{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DATA_602_Ensemble_Learning.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"exyEa4AQT9ZC"},"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.datasets import make_blobs\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n","    random_state=0)\n","\n","clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n","    random_state=0)\n","scores = cross_val_score(clf, X, y, cv=5)\n","scores.mean()                               \n","\n","\n","clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n","    min_samples_split=2, random_state=0)\n","scores = cross_val_score(clf, X, y, cv=5)\n","scores.mean()                               \n","\n","\n","clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n","    min_samples_split=2, random_state=0)\n","scores = cross_val_score(clf, X, y, cv=5)\n","scores.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ycjE8xl4UGB8"},"source":["print(__doc__)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","%matplotlib inline\n","from sklearn.datasets import load_iris\n","from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n","                              AdaBoostClassifier)\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Parameters\n","n_classes = 3\n","n_estimators = 30\n","cmap = plt.cm.RdYlBu\n","plot_step = 0.02  # fine step width for decision surface contours\n","plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n","RANDOM_SEED = 13  # fix the seed on each iteration\n","\n","# Load data\n","iris = load_iris()\n","\n","plot_idx = 1\n","\n","models = [DecisionTreeClassifier(max_depth=None),\n","          RandomForestClassifier(n_estimators=n_estimators),\n","          ExtraTreesClassifier(n_estimators=n_estimators),\n","          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n","                             n_estimators=n_estimators)]\n","\n","for pair in ([0, 1], [0, 2], [2, 3]):\n","    for model in models:\n","        # We only take the two corresponding features\n","        X = iris.data[:, pair]\n","        y = iris.target\n","\n","        # Shuffle\n","        idx = np.arange(X.shape[0])\n","        np.random.seed(RANDOM_SEED)\n","        np.random.shuffle(idx)\n","        X = X[idx]\n","        y = y[idx]\n","\n","        # Standardize\n","        mean = X.mean(axis=0)\n","        std = X.std(axis=0)\n","        X = (X - mean) / std\n","\n","        # Train\n","        model.fit(X, y)\n","\n","        scores = model.score(X, y)\n","        # Create a title for each column and the console by using str() and\n","        # slicing away useless parts of the string\n","        model_title = str(type(model)).split(\n","            \".\")[-1][:-2][:-len(\"Classifier\")]\n","\n","        model_details = model_title\n","        if hasattr(model, \"estimators_\"):\n","            model_details += \" with {} estimators\".format(\n","                len(model.estimators_))\n","        print(model_details + \" with features\", pair,\n","              \"has a score of\", scores)\n","\n","        plt.subplot(3, 4, plot_idx)\n","        if plot_idx <= len(models):\n","            # Add a title at the top of each column\n","            plt.title(model_title, fontsize=9)\n","\n","        # Now plot the decision boundary using a fine mesh as input to a\n","        # filled contour plot\n","        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n","                             np.arange(y_min, y_max, plot_step))\n","\n","        # Plot either a single DecisionTreeClassifier or alpha blend the\n","        # decision surfaces of the ensemble of classifiers\n","        if isinstance(model, DecisionTreeClassifier):\n","            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n","            Z = Z.reshape(xx.shape)\n","            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n","        else:\n","            # Choose alpha blend level with respect to the number\n","            # of estimators\n","            # that are in use (noting that AdaBoost can use fewer estimators\n","            # than its maximum if it achieves a good enough fit early on)\n","            estimator_alpha = 1.0 / len(model.estimators_)\n","            for tree in model.estimators_:\n","                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n","                Z = Z.reshape(xx.shape)\n","                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n","\n","        # Build a coarser grid to plot a set of ensemble classifications\n","        # to show how these are different to what we see in the decision\n","        # surfaces. These points are regularly space and do not have a\n","        # black outline\n","        xx_coarser, yy_coarser = np.meshgrid(\n","            np.arange(x_min, x_max, plot_step_coarser),\n","            np.arange(y_min, y_max, plot_step_coarser))\n","        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n","                                         yy_coarser.ravel()]\n","                                         ).reshape(xx_coarser.shape)\n","        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n","                                c=Z_points_coarser, cmap=cmap,\n","                                edgecolors=\"none\")\n","\n","        # Plot the training points, these are clustered together and have a\n","        # black outline\n","        plt.scatter(X[:, 0], X[:, 1], c=y,\n","                    cmap=ListedColormap(['r', 'y', 'b']),\n","                    edgecolor='k', s=20)\n","        plot_idx += 1  # move on to the next plot in sequence\n","\n","plt.suptitle(\"Classifiers on feature subsets of the Iris dataset\", fontsize=12)\n","plt.axis(\"tight\")\n","plt.tight_layout(h_pad=0.2, w_pad=0.2, pad=2.5)\n","plt.show()"],"execution_count":null,"outputs":[]}]}